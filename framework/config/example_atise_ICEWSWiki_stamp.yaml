console:
  quiet: False
  format: {}
  echo: True
  log_level: info

#local
task:
  folder: /home/zhangjs/experiments/TKGbenchmark/results
  type: train
  device: 'cuda'
  task_form: stamp # original[s,r,?,t] stamp[s.?,?,t] span_end[s,r,t,t_s,?]
  reciprocal_training: true

random_seed:
  default: -1
  python: -1
  torch: -1
  numpy: -1
  numba: -1


# local
dataset:
  folder: /home/zhangjs/experiments/TKGbenchmark/datasets/ICEWSWiki

  # indexes mapping [false, true]
  # TODO 可以保存到cache
  mapping: False
  filter: False
  temporal:
    resolution: "day" # month for FinWiki; year for YAGO; day for WikiICEWS
    index: True
    float: False
    gran: 3
  args: ~

  name: 'stamp_span'
  num_entities: -1
  num_relations: -1
  pickle: True

negative_sampling:
  type: 'basic'
  num_samples: 50
  filter: False
  as_matrix: True
  target: ro #[head, tail, both]->original [ro]-> for stamp task [time]->for span_end task
  args: ~

model:
  type: 'atise'
  embedding_dim: 100
  cmin: 0.003
  cmax: 0.3
  args: ~




train:
  # Split used for training (specified under 'data.files').
  split: train
  type: negative_sampling

  loss:
    type: log_rank_loss
    gamma: 120
    temp: 0.5
    args: ~

  max_epochs: 100

  loader:
    num_workers: 10
    pin_memory: True
    drop_last: False
    timeout: 0

  valid:
    split: valid # in [test or valid]
    every: 10
    batch_size: 64
    subbatch_size: 32
    filter: time-aware  # in [off, static, time-aware]
    ordering: optimistic    # in [optimistic, peesimistic]
    k: [1,50,100,500]

    early_stopping:
      early_stop: True
      patience: 100
      epochs: 50
      metric_thresh: 0.2

  batch_size: 512
  subbatch_size: 64
  subbatch_adaptive: True
  optimizer:
    type: Adam
    args:
      lr: 0.001

    default:
      type: Adam

      args:
        +++: +++

  regularizer: ~
  inplace_regularizer:
    renorm:
      type: inplace_renorm_regularize
      p: 2
      dim: 0
      maxnorm: 1
      args: ~
    clamp:
      type: inplace_clamp_regularize
      min: 0.003
      max: 0.3
      args: ~

  lr_scheduler: ""

  lr_scheduler_args:
    +++: +++

  trace_level: epoch           # batch, epoch

  checkpoint:
    every: 1000
    keep: 3

  auto_correct: False
  abort_on_nan: True
  visualize_graph: False

eval:
  filter: time-aware
  ordering: ascending
  preference: optimistic
  k: [1,50,100,500]


hpo:
  num_workers: -1
  num_trials: 40
  num_random_trials: 20
  hyperparam:
    - name: model.embedding_dim
      type: choice
      values: [512, 128, 256, 1024]
      value_type: int
    - name: train.optimizer.args.lr
      type: range
      bounds: [0.0000001, 0.001]
      value_type: float
    - name: train.loss.type
      type: choice
      values: [cross_entropy_loss', 'log_rank_loss']
      value_type: str


    # phase2
#    - name: model.embedding.global.init
#      type: choice
#      values: ['xavier_uniform', 'xavier_normal']
#      value_type: str
#    - name: train.n3.weight
#      type: range
#      bounds: [0.0, 0.1]
#      value_type: float
#    - name: train.lambda3.weight
#      type: range
#      bounds: [0.0, 0.1]
#      value_type: float


